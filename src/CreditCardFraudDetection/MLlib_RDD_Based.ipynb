{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.mllib.linalg import DenseVector\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD, LogisticRegressionWithLBFGS\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics, BinaryClassificationMetrics\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "    .appName(\"credit-card-fraud-detection\")\\\n",
    "    .master(\"local[*]\")\\\n",
    "    .config(\"spark.log.level\", \"ERROR\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Change the path to the CSV file as needed\n",
    "# Load the dataset\n",
    "df = spark.read.csv(\"../../data/creditcard.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding the data**:\n",
    "- According to the dataset description, the input variables are the result of a PCA transformation except \"Time\" and \"Amount\" so the features are previously scaled. \n",
    "- Every value in the dataset is not null so imputing is also not needed.\n",
    "- The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions. To deal with this problem, we have 2 methods:\n",
    "    - Cost-sensitive learning: the lost function will be adjusted to favor the detection of the minority class.\n",
    "    - Undersampling, oversampling technique or a combination of the two.\n",
    "\n",
    "Because of the reasons above and the fact that I will choose the cost-sensitive learning method to deal with the highly unbalanced nature of the dataset, this data processing step will include:\n",
    "- Adding a weight column of value 0.99828 whenever the label is 1 (minority) and 0.00172 when the label is 0 (majority) \n",
    "- Using the VectorAssembler class to assemble feature columns into a single vector column\n",
    "- Splitting the dataset into train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use all columns as features exclude the target column \"Class\"\n",
    "input_cols = df.columns[:-1]\n",
    "\n",
    "# Assemble the features into a single vector column\n",
    "assembler = VectorAssembler(inputCols=input_cols, outputCol=\"features\")\n",
    "df = assembler.transform(df).select(\"features\", \"Class\")\n",
    "\n",
    "# Sample training data in a stratified fashion\n",
    "train_df = df.sampleBy(\"Class\", {1: 0.8, 0: 0.8}, seed=42)\n",
    "\n",
    "# Get test data as the remaining set\n",
    "test_df = df.subtract(train_df)\n",
    "\n",
    "# Oversample the train df to deal with class imbalance\n",
    "# Calculate class counts in the training data\n",
    "class_counts = train_df.groupBy(\"Class\").count().collect()\n",
    "major_count = next((row['count'] for row in class_counts if row['Class'] == 0), 0)\n",
    "minor_count = next((row['count'] for row in class_counts if row['Class'] == 1), 0)\n",
    "# Calculate the desired oversampling ratio\n",
    "ratio = float(major_count) / minor_count\n",
    "# Filter out and oversample the minor class \n",
    "oversampled_minor_df = train_df\\\n",
    "    .filter(col(\"Class\") == 1)\\\n",
    "    .sample(withReplacement=True, fraction=ratio, seed=42)\n",
    "# Combine the minor into the train df\n",
    "train_df = train_df\\\n",
    "    .filter(col(\"Class\") == 0)\\\n",
    "    .union(oversampled_minor_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Logistic Regression model using spark.mllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Convert the DataFrame into an RDD of LabeledPoint objects\n",
    "train_rdd = train_df.rdd.map(lambda row: LabeledPoint(row.Class, DenseVector(row.features.values)))\n",
    "\n",
    "# Train the logistic regression model\n",
    "model = LogisticRegressionWithLBFGS.train(train_rdd, intercept=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients:  [-1.1301222137139996e-05,1.0140537832385095,0.333050459631398,0.3343750167040232,0.9893938495441409,0.8300514129056135,-0.5279541236002336,-0.881900487720863,-0.5063825244715748,-0.8930615126383927,-1.7094928320742324,0.4033388749816862,-1.136705692059036,-0.3757678199445609,-1.4835055998944386,-0.22548697721935382,-0.9963148350091529,-1.2125049509150228,-0.22651769134432215,0.6323593306962435,-1.5160773153815292,0.2801917728569362,0.8997841827896961,0.448158197226758,-0.3352580099842403,-0.1605665927611555,-0.3755132181271749,-1.3638526068793428,0.22746495942303743,0.00945518194920525]\n",
      "Intercept:  -3.6508136814470724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/keineik/Projects/lab03-spark-ml/venv/lib64/python3.13/site-packages/pyspark/sql/context.py:158: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9769013283069865\n",
      "Precision: {0.0: 0.9998541689452769, 1.0: 0.06381435823060189}\n",
      "Recall: {0.0: 0.977004328387453, 1.0: 0.9166666666666666}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1024:=================================================>    (11 + 1) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under ROC: 0.9468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"Coefficients: \", model.weights)\n",
    "print(\"Intercept: \", model.intercept)\n",
    "\n",
    "test_rdd = test_df.rdd.map(lambda row: LabeledPoint(row.Class, DenseVector(row.features.values)))\n",
    "predictionAndLabels = test_rdd.map(lambda p: (float(model.predict(p.features)), p.label))\n",
    "multiMetrics = MulticlassMetrics(predictionAndLabels)\n",
    "\n",
    "# Overall accuracy\n",
    "accuracy = multiMetrics.accuracy\n",
    "\n",
    "# Precision and recall for each label\n",
    "labels = predictionAndLabels.map(lambda x: x[1]).distinct().collect()\n",
    "precision_by_label = {label: multiMetrics.precision(label) for label in labels}\n",
    "recall_by_label = {label: multiMetrics.recall(label) for label in labels}\n",
    "\n",
    "print(\"Accuracy: {}\".format(accuracy))\n",
    "print(\"Precision:\", precision_by_label)\n",
    "print(\"Recall:\", recall_by_label)\n",
    "\n",
    "# Calculate the area under the ROC curve\n",
    "binaryMetrics = BinaryClassificationMetrics(predictionAndLabels)\n",
    "roc_auc = binaryMetrics.areaUnderROC\n",
    "print(\"Area under ROC: {:.4f}\".format(roc_auc))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
