{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.mllib.linalg import DenseVector\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.classification import LogisticRegressionWithSGD, LogisticRegressionWithLBFGS\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics, BinaryClassificationMetrics\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "    .appName(\"credit-card-fraud-detection\")\\\n",
    "    .master(\"local[*]\")\\\n",
    "    .config(\"spark.log.level\", \"ERROR\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+-------------------+----------------+------------------+-------------------+-------------------+-------------------+------------------+------------------+-------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+-------------------+--------------------+-------------------+------------------+------------------+------------------+------------------+--------------------+-------------------+------+-----+\n",
      "|Time|                V1|                 V2|              V3|                V4|                 V5|                 V6|                 V7|                V8|                V9|                V10|               V11|               V12|               V13|               V14|               V15|               V16|               V17|                V18|               V19|                V20|                 V21|                V22|               V23|               V24|               V25|               V26|                 V27|                V28|Amount|Class|\n",
      "+----+------------------+-------------------+----------------+------------------+-------------------+-------------------+-------------------+------------------+------------------+-------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+-------------------+--------------------+-------------------+------------------+------------------+------------------+------------------+--------------------+-------------------+------+-----+\n",
      "| 0.0|  -1.3598071336738|-0.0727811733098497|2.53634673796914|  1.37815522427443| -0.338320769942518|  0.462387777762292|  0.239598554061257|0.0986979012610507| 0.363786969611213| 0.0907941719789316|-0.551599533260813|-0.617800855762348|-0.991389847235408|-0.311169353699879|  1.46817697209427|-0.470400525259478| 0.207971241929242| 0.0257905801985591| 0.403992960255733|  0.251412098239705|  -0.018306777944153|  0.277837575558899|-0.110473910188767|0.0669280749146731| 0.128539358273528|-0.189114843888824|   0.133558376740387|-0.0210530534538215|149.62|    0|\n",
      "| 0.0|  1.19185711131486|   0.26615071205963|0.16648011335321| 0.448154078460911| 0.0600176492822243|-0.0823608088155687|-0.0788029833323113|0.0851016549148104|-0.255425128109186| -0.166974414004614|  1.61272666105479|  1.06523531137287|  0.48909501589608|-0.143772296441519| 0.635558093258208| 0.463917041022171|-0.114804663102346| -0.183361270123994|-0.145783041325259|-0.0690831352230203|  -0.225775248033138| -0.638671952771851| 0.101288021253234|-0.339846475529127| 0.167170404418143| 0.125894532368176|-0.00898309914322813| 0.0147241691924927|  2.69|    0|\n",
      "| 1.0| -1.35835406159823|  -1.34016307473609|1.77320934263119| 0.379779593034328| -0.503198133318193|   1.80049938079263|  0.791460956450422| 0.247675786588991| -1.51465432260583|  0.207642865216696| 0.624501459424895| 0.066083685268831| 0.717292731410831|-0.165945922763554|  2.34586494901581| -2.89008319444231|  1.10996937869599| -0.121359313195888| -2.26185709530414|  0.524979725224404|   0.247998153469754|  0.771679401917229| 0.909412262347719|-0.689280956490685|-0.327641833735251|-0.139096571514147| -0.0553527940384261|-0.0597518405929204|378.66|    0|\n",
      "| 1.0|-0.966271711572087| -0.185226008082898|1.79299333957872|-0.863291275036453|-0.0103088796030823|   1.24720316752486|   0.23760893977178| 0.377435874652262| -1.38702406270197|-0.0549519224713749|-0.226487263835401| 0.178228225877303| 0.507756869957169| -0.28792374549456|-0.631418117709045|  -1.0596472454325|-0.684092786345479|   1.96577500349538|  -1.2326219700892| -0.208037781160366|  -0.108300452035545|0.00527359678253453|-0.190320518742841| -1.17557533186321| 0.647376034602038|-0.221928844458407|  0.0627228487293033| 0.0614576285006353| 123.5|    0|\n",
      "| 2.0| -1.15823309349523|  0.877736754848451|  1.548717846511| 0.403033933955121| -0.407193377311653| 0.0959214624684256|  0.592940745385545|-0.270532677192282| 0.817739308235294|  0.753074431976354|-0.822842877946363|  0.53819555014995|   1.3458515932154| -1.11966983471731| 0.175121130008994|-0.451449182813529|-0.237033239362776|-0.0381947870352842| 0.803486924960175|  0.408542360392758|-0.00943069713232919|   0.79827849458971|-0.137458079619063| 0.141266983824769|-0.206009587619756| 0.502292224181569|   0.219422229513348|  0.215153147499206| 69.99|    0|\n",
      "+----+------------------+-------------------+----------------+------------------+-------------------+-------------------+-------------------+------------------+------------------+-------------------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-------------------+------------------+-------------------+--------------------+-------------------+------------------+------------------+------------------+------------------+--------------------+-------------------+------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Change the path to the CSV file as needed\n",
    "# Load the dataset\n",
    "df = spark.read.csv(\"../../data/creditcard.csv\", header=True, inferSchema=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding the data**:\n",
    "- According to the dataset description, the input variables are the result of a PCA transformation except \"Time\" and \"Amount\" so the features are previously scaled. \n",
    "- Every value in the dataset is not null so imputing is also not needed.\n",
    "- The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions. To deal with this problem, we have 2 methods:\n",
    "    - Cost-sensitive learning: the lost function will be adjusted to favor the detection of the minority class.\n",
    "    - Undersampling, oversampling technique or a combination of the two.\n",
    "\n",
    "Because of the reasons above and the fact that I will choose the cost-sensitive learning method to deal with the highly unbalanced nature of the dataset, this data processing step will include:\n",
    "- Adding a weight column of value 0.99828 whenever the label is 1 (minority) and 0.00172 when the label is 0 (majority) \n",
    "- Using the VectorAssembler class to assemble feature columns into a single vector column\n",
    "- Splitting the dataset into train and test set.\n",
    "\n",
    "When using DataFrame-based MLlib, the model will standardize the Time and Amount column first. With the RDD-based MLlib, this is not the case so I will need to standardize them by myself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize Time and Amount using min-max scaling\n",
    "# Compute statistics\n",
    "time_stats = df.agg(min(\"Time\").alias(\"min_time\"), F.max(\"Time\").alias(\"max_time\")).collect()[0]\n",
    "amount_stats = df.agg(min(\"Amount\").alias(\"min_amount\"), F.max(\"Amount\").alias(\"max_amount\")).collect()[0]\n",
    "\n",
    "# Apply normalization (Min-Max scaling)\n",
    "df = df.withColumn(\"Time\", \n",
    "                  (col(\"Time\") - time_stats[\"min_time\"]) / \n",
    "                  (time_stats[\"max_time\"] - time_stats[\"min_time\"]))\n",
    "\n",
    "df = df.withColumn(\"Amount\", \n",
    "                  (col(\"Amount\") - amount_stats[\"min_amount\"]) / \n",
    "                  (amount_stats[\"max_amount\"] - amount_stats[\"min_amount\"]))\n",
    "\n",
    "# Use all columns as features exclude the target column \"Class\"\n",
    "input_cols = df.columns[:-1]\n",
    "\n",
    "# Assemble the features into a single vector column\n",
    "assembler = VectorAssembler(inputCols=input_cols, outputCol=\"features\")\n",
    "df = assembler.transform(df).select(\"features\", \"Class\")\n",
    "\n",
    "# Sample training data in a stratified fashion\n",
    "train_df = df.sampleBy(\"Class\", {1: 0.8, 0: 0.8}, seed=42)\n",
    "\n",
    "# Get test data as the remaining set\n",
    "test_df = df.subtract(train_df)\n",
    "\n",
    "# Oversample the train df to deal with class imbalance\n",
    "# Calculate class counts in the training data\n",
    "class_counts = train_df.groupBy(\"Class\").count().collect()\n",
    "major_count = next((row['count'] for row in class_counts if row['Class'] == 0), 0)\n",
    "minor_count = next((row['count'] for row in class_counts if row['Class'] == 1), 0)\n",
    "# Calculate the desired oversampling ratio\n",
    "ratio = float(major_count) / minor_count\n",
    "# Filter out and oversample the minor class \n",
    "oversampled_minor_df = train_df\\\n",
    "    .filter(col(\"Class\") == 1)\\\n",
    "    .sample(withReplacement=True, fraction=ratio, seed=42)\n",
    "# Combine the minor into the train df\n",
    "train_df = train_df\\\n",
    "    .filter(col(\"Class\") == 0)\\\n",
    "    .union(oversampled_minor_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Logistic Regression model using spark.mllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Convert the DataFrame into an RDD of LabeledPoint objects\n",
    "train_rdd = train_df.rdd.map(lambda row: LabeledPoint(row.Class, DenseVector(row.features.values)))\n",
    "\n",
    "# Train the logistic regression model\n",
    "# model = LogisticRegressionWithLBFGS.train(train_rdd, intercept=True)\n",
    "\n",
    "# Train the model using SGD\n",
    "model = LogisticRegressionWithSGD.train(train_rdd, step=1, intercept=True, iterations=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients:  [-1.0409482336703346,-0.1554068524341427,0.09340311065745453,-0.49937226452937206,0.559723197394129,-0.14484663190704042,-0.13618982707538577,-0.3976244866232469,-0.05499241701336593,-0.2694559188324656,-0.6083801909877051,0.3866671931784822,-0.7631844695985326,-0.10960052671668699,-0.9691166940532628,-0.05689768392003591,-0.5887090517109573,-0.8685733113731122,-0.24887362257688883,0.09090664606598867,0.06396430777819669,0.13915508760604234,0.08416415903967822,-0.08522876820847165,-0.03994130565569177,-0.007095639373137107,-0.04339372072375452,0.001447579999732893,0.01102788190649151,-0.0050911726274490594]\n",
      "Intercept:  -1.0827907889518473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9768835464196169\n",
      "Precision: [0.999817721149815, 0.0625]\n",
      "Recall: [0.9770221406814984, 0.8958333333333334]\n",
      "Area under ROC: 0.9364\n",
      "Area under PR: 0.0593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"Coefficients: \", model.weights)\n",
    "print(\"Intercept: \", model.intercept)\n",
    "\n",
    "test_rdd = test_df.rdd.map(lambda row: LabeledPoint(row.Class, DenseVector(row.features.values)))\n",
    "predictionAndLabels = test_rdd.map(lambda p: (float(model.predict(p.features)), p.label))\n",
    "\n",
    "# Calculate accuracy, precision, and recall\n",
    "multiMetrics = MulticlassMetrics(predictionAndLabels)\n",
    "accuracy = multiMetrics.accuracy\n",
    "labels = predictionAndLabels.map(lambda x: x[1]).distinct().collect()\n",
    "precision_by_label = {label: multiMetrics.precision(label) for label in labels}\n",
    "recall_by_label = {label: multiMetrics.recall(label) for label in labels}\n",
    "\n",
    "print(\"Accuracy: {}\".format(accuracy))\n",
    "print(\"Precision:\", list(precision_by_label.values()))\n",
    "print(\"Recall:\", list(recall_by_label.values()))\n",
    "\n",
    "# Calculate the area under the ROC curve and PR curve\n",
    "binaryMetrics = BinaryClassificationMetrics(predictionAndLabels)\n",
    "roc_auc = binaryMetrics.areaUnderROC\n",
    "roc_pr = binaryMetrics.areaUnderPR\n",
    "\n",
    "print(\"Area under ROC: {:.4f}\".format(roc_auc))\n",
    "print(\"Area under PR: {:.4f}\".format(roc_pr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment with parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting hyperparameter tuning with 4-fold cross-validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/keineik/Projects/lab03-spark-ml/venv/lib64/python3.13/site-packages/pyspark/mllib/classification.py:395: FutureWarning: Deprecated in 2.0.0. Use ml.classification.LogisticRegression or LogisticRegressionWithLBFGS.\n",
      "  warnings.warn(\n",
      "/home/keineik/Projects/lab03-spark-ml/venv/lib64/python3.13/site-packages/pyspark/sql/context.py:158: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step size: 5, Iterations: 20, Average Accuracy: 0.9743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step size: 5, Iterations: 50, Average Accuracy: 0.9774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step size: 5, Iterations: 100, Average Accuracy: 0.9772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step size: 5, Iterations: 200, Average Accuracy: 0.9772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step size: 1.0, Iterations: 20, Average Accuracy: 0.9422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step size: 1.0, Iterations: 50, Average Accuracy: 0.9711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step size: 1.0, Iterations: 100, Average Accuracy: 0.9752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step size: 1.0, Iterations: 200, Average Accuracy: 0.9765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step size: 0.1, Iterations: 20, Average Accuracy: 0.0018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                ]]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step size: 0.1, Iterations: 50, Average Accuracy: 0.0019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                ]]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step size: 0.1, Iterations: 100, Average Accuracy: 0.0671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step size: 0.1, Iterations: 200, Average Accuracy: 0.5153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                2]]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step size: 0.01, Iterations: 20, Average Accuracy: 0.0018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                ]]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step size: 0.01, Iterations: 50, Average Accuracy: 0.0018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.===========>   (13 + 1) / 14]]]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/keineik/Projects/lab03-spark-ml/venv/lib64/python3.13/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/keineik/Projects/lab03-spark-ml/venv/lib64/python3.13/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/usr/lib64/python3.13/socket.py\", line 719, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ~~~~~~~~~~~~~~~~~~~~^^^\n",
      "KeyboardInterrupt\n",
      "25/04/09 21:29:54 ERROR Executor: Exception in task 13.0 in stage 118034.0 (TID 356337)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/keineik/Projects/lab03-spark-ml/venv/lib/python3.13/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "    ~~~~~~~^^\n",
      "  File \"/home/keineik/Projects/lab03-spark-ml/venv/lib/python3.13/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/keineik/Projects/lab03-spark-ml/venv/lib/python3.13/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "  File \"/home/keineik/Projects/lab03-spark-ml/venv/lib/python3.13/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 152, in load_stream\n",
      "    yield self._read_with_length(stream)\n",
      "          ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^\n",
      "  File \"/home/keineik/Projects/lab03-spark-ml/venv/lib/python3.13/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "           ~~~~~~~~~~^^^^^\n",
      "  File \"/home/keineik/Projects/lab03-spark-ml/venv/lib/python3.13/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "           ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/keineik/Projects/lab03-spark-ml/venv/lib/python3.13/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 2208, in <lambda>\n",
      "    return lambda *a: dataType.fromInternal(a)\n",
      "                      ~~~~~~~~~~~~~~~~~~~~~^^^\n",
      "  File \"/home/keineik/Projects/lab03-spark-ml/venv/lib/python3.13/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1093, in fromInternal\n",
      "    return _create_row(self.names, values)\n",
      "  File \"/home/keineik/Projects/lab03-spark-ml/venv/lib/python3.13/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 2214, in _create_row\n",
      "    row = Row(*values)\n",
      "  File \"/home/keineik/Projects/lab03-spark-ml/venv/lib/python3.13/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 2297, in __new__\n",
      "    return tuple.__new__(cls, args)\n",
      "           ~~~~~~~~~~~~~^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\n",
      "\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1597)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1524)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1588)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1389)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdateRDDBlock(BlockManager.scala:1343)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:379)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "25/04/09 21:29:54 ERROR TaskSetManager: Task 13 in stage 118034.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[71]\u001b[39m\u001b[32m, line 56\u001b[39m\n\u001b[32m     53\u001b[39m validation_fold_rdd = validation_df.rdd.map(\u001b[38;5;28;01mlambda\u001b[39;00m row: LabeledPoint(row.Class, DenseVector(row.features.values)))\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# Train model with current parameters\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m model = \u001b[43mLogisticRegressionWithSGD\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_fold_rdd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43miterations\u001b[49m\u001b[43m=\u001b[49m\u001b[43miterations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstep_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m    \u001b[49m\u001b[43mintercept\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# Evaluate on validation data using accuracy instead of precision\u001b[39;00m\n\u001b[32m     64\u001b[39m predictionAndLabels = validation_fold_rdd.map(\u001b[38;5;28;01mlambda\u001b[39;00m p: (\u001b[38;5;28mfloat\u001b[39m(model.predict(p.features)), p.label))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/lab03-spark-ml/venv/lib64/python3.13/site-packages/pyspark/mllib/classification.py:416\u001b[39m, in \u001b[36mLogisticRegressionWithSGD.train\u001b[39m\u001b[34m(cls, data, iterations, step, miniBatchFraction, initialWeights, regParam, regType, intercept, validateData, convergenceTol)\u001b[39m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain\u001b[39m(rdd: RDD[LabeledPoint], i: Vector) -> Iterable[Any]:\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m callMLlibFunc(\n\u001b[32m    403\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mtrainLogisticRegressionModelWithSGD\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    404\u001b[39m         rdd,\n\u001b[32m   (...)\u001b[39m\u001b[32m    413\u001b[39m         \u001b[38;5;28mfloat\u001b[39m(convergenceTol),\n\u001b[32m    414\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m416\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_regression_train_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLogisticRegressionModel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitialWeights\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/lab03-spark-ml/venv/lib64/python3.13/site-packages/pyspark/mllib/regression.py:279\u001b[39m, in \u001b[36m_regression_train_wrapper\u001b[39m\u001b[34m(train_func, modelClass, data, initial_weights)\u001b[39m\n\u001b[32m    277\u001b[39m     initial_weights = [\u001b[32m0.0\u001b[39m] * \u001b[38;5;28mlen\u001b[39m(data.first().features)\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m modelClass == LogisticRegressionModel:\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m     weights, intercept, numFeatures, numClasses = \u001b[43mtrain_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_convert_to_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    282\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m modelClass(weights, intercept, numFeatures, numClasses)  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/lab03-spark-ml/venv/lib64/python3.13/site-packages/pyspark/mllib/classification.py:402\u001b[39m, in \u001b[36mLogisticRegressionWithSGD.train.<locals>.train\u001b[39m\u001b[34m(rdd, i)\u001b[39m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain\u001b[39m(rdd: RDD[LabeledPoint], i: Vector) -> Iterable[Any]:\n\u001b[32m--> \u001b[39m\u001b[32m402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallMLlibFunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrainLogisticRegressionModelWithSGD\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrdd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterations\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mminiBatchFraction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mregParam\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m        \u001b[49m\u001b[43mregType\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mintercept\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalidateData\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconvergenceTol\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/lab03-spark-ml/venv/lib64/python3.13/site-packages/pyspark/mllib/common.py:139\u001b[39m, in \u001b[36mcallMLlibFunc\u001b[39m\u001b[34m(name, *args)\u001b[39m\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m sc._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    138\u001b[39m api = \u001b[38;5;28mgetattr\u001b[39m(sc._jvm.PythonMLLibAPI(), name)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallJavaFunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43msc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/lab03-spark-ml/venv/lib64/python3.13/site-packages/pyspark/mllib/common.py:131\u001b[39m, in \u001b[36mcallJavaFunc\u001b[39m\u001b[34m(sc, func, *args)\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Call Java Function\"\"\"\u001b[39;00m\n\u001b[32m    130\u001b[39m java_args = [_py2java(sc, a) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _java2py(sc, \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mjava_args\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/lab03-spark-ml/venv/lib64/python3.13/site-packages/py4j/java_gateway.py:1321\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1314\u001b[39m args_command, temp_args = \u001b[38;5;28mself\u001b[39m._build_args(*args)\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m-> \u001b[39m\u001b[32m1321\u001b[39m answer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1322\u001b[39m return_value = get_return_value(\n\u001b[32m   1323\u001b[39m     answer, \u001b[38;5;28mself\u001b[39m.gateway_client, \u001b[38;5;28mself\u001b[39m.target_id, \u001b[38;5;28mself\u001b[39m.name)\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/lab03-spark-ml/venv/lib64/python3.13/site-packages/py4j/java_gateway.py:1038\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1036\u001b[39m connection = \u001b[38;5;28mself\u001b[39m._get_connection()\n\u001b[32m   1037\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1038\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1039\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[32m   1040\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m._create_connection_guard(connection)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/lab03-spark-ml/venv/lib64/python3.13/site-packages/py4j/clientserver.py:511\u001b[39m, in \u001b[36mClientServerConnection.send_command\u001b[39m\u001b[34m(self, command)\u001b[39m\n\u001b[32m    509\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    510\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m511\u001b[39m         answer = smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:-\u001b[32m1\u001b[39m])\n\u001b[32m    512\u001b[39m         logger.debug(\u001b[33m\"\u001b[39m\u001b[33mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m\"\u001b[39m.format(answer))\n\u001b[32m    513\u001b[39m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[32m    514\u001b[39m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib64/python3.13/socket.py:719\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    717\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mcannot read from timed out object\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m719\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    721\u001b[39m     \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Perform hyperparameter tuning for step_size and max_iterations in SGD\n",
    "# Define parameters to test\n",
    "step_sizes = [5, 1.0, 0.1, 0.01]\n",
    "max_iterations_list = [20, 50, 100, 200]\n",
    "k_folds = 4\n",
    "\n",
    "# Sample training data in a stratified fashion WITHOUT oversampling yet\n",
    "# We'll use the original df to create train/test split\n",
    "train_df = df.sampleBy(\"Class\", {1: 0.8, 0: 0.8}, seed=42)\n",
    "test_df = df.subtract(train_df)\n",
    "\n",
    "# Store results\n",
    "results = {}\n",
    "\n",
    "print(f\"Starting hyperparameter tuning with {k_folds}-fold cross-validation...\")\n",
    "for step_size in step_sizes:\n",
    "    for iterations in max_iterations_list:\n",
    "        accuracy_values = []\n",
    "        \n",
    "        # Key for storing results\n",
    "        param_key = (step_size, iterations)\n",
    "        \n",
    "        # Perform k-fold cross-validation using sampleBy and subtract\n",
    "        for i in range(k_folds):\n",
    "            # Create a stratified validation fold (1/k of the data)\n",
    "            # Using different seed for each fold\n",
    "            validation_df = train_df.sampleBy(\"Class\", {1: 1.0/k_folds, 0: 1.0/k_folds}, seed=42+i)\n",
    "            \n",
    "            # Create training fold as the remaining data\n",
    "            train_fold_df = train_df.subtract(validation_df)\n",
    "            \n",
    "            # Now apply oversampling to just this training fold\n",
    "            # Calculate class counts in the training fold\n",
    "            fold_class_counts = train_fold_df.groupBy(\"Class\").count().collect()\n",
    "            fold_major_count = next((row['count'] for row in fold_class_counts if row['Class'] == 0), 0)\n",
    "            fold_minor_count = next((row['count'] for row in fold_class_counts if row['Class'] == 1), 0)\n",
    "            \n",
    "            # Calculate the desired oversampling ratio for this fold\n",
    "            fold_ratio = float(fold_major_count) / fold_minor_count\n",
    "            \n",
    "            # Oversample the minority class in this fold\n",
    "            oversampled_minor_df = train_fold_df\\\n",
    "                .filter(col(\"Class\") == 1)\\\n",
    "                .sample(withReplacement=True, fraction=fold_ratio, seed=42+i)\n",
    "                \n",
    "            # Create balanced training fold\n",
    "            balanced_train_fold_df = train_fold_df\\\n",
    "                .filter(col(\"Class\") == 0)\\\n",
    "                .union(oversampled_minor_df)\n",
    "            \n",
    "            # Convert to RDD format for model training\n",
    "            train_fold_rdd = balanced_train_fold_df.rdd.map(lambda row: LabeledPoint(row.Class, DenseVector(row.features.values)))\n",
    "            validation_fold_rdd = validation_df.rdd.map(lambda row: LabeledPoint(row.Class, DenseVector(row.features.values)))\n",
    "            \n",
    "            # Train model with current parameters\n",
    "            model = LogisticRegressionWithSGD.train(\n",
    "                train_fold_rdd, \n",
    "                iterations=iterations,\n",
    "                step=step_size, \n",
    "                intercept=True,\n",
    "            )\n",
    "            \n",
    "            # Evaluate on validation data using accuracy instead of precision\n",
    "            predictionAndLabels = validation_fold_rdd.map(lambda p: (float(model.predict(p.features)), p.label))\n",
    "            \n",
    "            # Use MulticlassMetrics to get accuracy\n",
    "            multiMetrics = MulticlassMetrics(predictionAndLabels)\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            accuracy = multiMetrics.accuracy\n",
    "            accuracy_values.append(accuracy)\n",
    "        \n",
    "        # Calculate average accuracy across folds\n",
    "        avg_accuracy = sum(accuracy_values) / len(accuracy_values)\n",
    "        results[param_key] = avg_accuracy\n",
    "        \n",
    "        print(f\"Step size: {step_size}, Iterations: {iterations}, Average Accuracy: {avg_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters:\n",
      "Step size: 5\n",
      "Iterations: 50\n",
      "Best average accuracy: 0.9774\n",
      "\n",
      "Training final SGD model with step size = 5, iterations = 50...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SGD Model Evaluation (with best step size = 5, iterations = 50):\n",
      "Accuracy: 0.9788\n",
      "Precision: {0.0: '0.9998', 1.0: '0.0685'}\n",
      "Recall: {0.0: '0.9789', 1.0: '0.9062'}\n",
      "Area under ROC: 0.9426\n",
      "Area under PR: 0.0654\n",
      "\n",
      "Comparison with LBFGS model:\n",
      "SGD (step_size = 5, iterations = 50) Precision: 0.0685\n",
      "SGD AUC: 0.9426\n",
      "LBFGS AUC: 0.9364\n"
     ]
    }
   ],
   "source": [
    "# Find best parameters based on accuracy\n",
    "best_accuracy = -1\n",
    "best_params = None\n",
    "for params, accuracy in results.items():\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_params = params\n",
    "\n",
    "# Extract individual parameters\n",
    "best_step_size, best_iterations = best_params\n",
    "\n",
    "print(\"\\nBest parameters:\")\n",
    "print(f\"Step size: {best_step_size}\")\n",
    "print(f\"Iterations: {best_iterations}\")\n",
    "print(f\"Best average accuracy: {best_accuracy:.4f}\")\n",
    "\n",
    "# Train final model with best parameters\n",
    "print(f\"\\nTraining final SGD model with step size = {best_step_size}, iterations = {best_iterations}...\")\n",
    "sgd_model = LogisticRegressionWithSGD.train(\n",
    "    train_rdd,\n",
    "    iterations=best_iterations,\n",
    "    step=best_step_size,\n",
    "    intercept=True,\n",
    ")\n",
    "\n",
    "# Evaluate on test set\n",
    "sgd_predictionAndLabels = test_rdd.map(lambda p: (float(sgd_model.predict(p.features)), p.label))\n",
    "\n",
    "# Calculate metrics\n",
    "sgd_multiMetrics = MulticlassMetrics(sgd_predictionAndLabels)\n",
    "sgd_binaryMetrics = BinaryClassificationMetrics(sgd_predictionAndLabels)\n",
    "\n",
    "# Print evaluation metrics\n",
    "sgd_accuracy = sgd_multiMetrics.accuracy\n",
    "sgd_labels = sgd_predictionAndLabels.map(lambda x: x[1]).distinct().collect()\n",
    "sgd_precision_by_label = {label: sgd_multiMetrics.precision(label) for label in sgd_labels}\n",
    "sgd_recall_by_label = {label: sgd_multiMetrics.recall(label) for label in sgd_labels}\n",
    "sgd_roc_auc = sgd_binaryMetrics.areaUnderROC\n",
    "sgd_roc_pr = sgd_binaryMetrics.areaUnderPR\n",
    "\n",
    "print(f\"\\nSGD Model Evaluation (with best step size = {best_step_size}, iterations = {best_iterations}):\")\n",
    "print(f\"Accuracy: {sgd_accuracy:.4f}\")\n",
    "print(\"Precision:\", {l: f\"{p:.4f}\" for l, p in sgd_precision_by_label.items()})\n",
    "print(\"Recall:\", {l: f\"{r:.4f}\" for l, r in sgd_recall_by_label.items()})\n",
    "print(f\"Area under ROC: {sgd_roc_auc:.4f}\")\n",
    "print(f\"Area under PR: {sgd_roc_pr:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
