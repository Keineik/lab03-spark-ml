{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from math import exp\n",
    "from pyspark.rdd import RDD\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "    .appName(\"credit-card-fraud-detection\")\\\n",
    "    .master(\"local[*]\")\\\n",
    "    .config(\"spark.log.level\", \"ERROR\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the path to the CSV file as needed\n",
    "# Load the CSV file as a text file and filter out the header\n",
    "lines = sc.textFile(\"../../data/creditcard.csv\")\n",
    "header = lines.first()\n",
    "data_rdd = lines.filter(lambda line: line != header)\n",
    "\n",
    "# Parse each line: split by comma and convert each element to float\n",
    "data_rdd = data_rdd.map(lambda line: [float(x.strip(\"\\\"\")) for x in line.split(\",\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding the data**:\n",
    "- According to the dataset description, the input variables are the result of a PCA transformation except \"Time\" and \"Amount\" so the features are previously scaled. \n",
    "- Every value in the dataset is not null so imputing is also not needed.\n",
    "- The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions. To deal with this problem, we have 2 methods:\n",
    "    - Cost-sensitive learning: the lost function will be adjusted to favor the detection of the minority class.\n",
    "    - Undersampling, oversampling technique or a combination of the two.\n",
    "\n",
    "Because of the reasons above and the fact that I will choose the oversampling method to deal with the highly unbalanced nature of the dataset, this data processing step will include:\n",
    "- Create an RDD where each record is a tuple (label, features)\n",
    "- Splitting the dataset into train and test set.\n",
    "- Oversample the minority class (Class = 1) \n",
    "\n",
    "When using DataFrame-based MLlib, the model will standardize the Time and Amount column first. With the low-level implementation, obviously this is not the case so I will need to standardize them by myself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create an RDD where each record is a tuple (label, features)\n",
    "data_rdd = data_rdd.map(lambda x: (x[-1], x[:-1]))\n",
    "\n",
    "# Define indices for Time and Amount columns\n",
    "time_idx = 0 \n",
    "amount_idx = 29\n",
    "\n",
    "# Calculate min and max values for scaling\n",
    "time_min = data_rdd.map(lambda x: x[1][time_idx]).min()\n",
    "time_max = data_rdd.map(lambda x: x[1][time_idx]).max()\n",
    "amount_min = data_rdd.map(lambda x: x[1][amount_idx]).min()\n",
    "amount_max = data_rdd.map(lambda x: x[1][amount_idx]).max()\n",
    "\n",
    "# Apply min-max scaling to Time and Amount columns\n",
    "def scale_time_amount(row):\n",
    "    label, features = row\n",
    "    features_copy = features.copy()\n",
    "    # Scale Time: (x - min) / (max - min)\n",
    "    features_copy[time_idx] = (features[time_idx] - time_min) / (time_max - time_min) \n",
    "    # Scale Amount: (x - min) / (max - min)\n",
    "    features_copy[amount_idx] = (features[amount_idx] - amount_min) / (amount_max - amount_min)\n",
    "    return (label, features_copy)\n",
    "\n",
    "# Apply the scaling\n",
    "data_rdd = data_rdd.map(scale_time_amount)\n",
    "\n",
    "# Split the data into train and test sets in a stratified fashion\n",
    "# Convert features to tuples (immutable) for the subtract operation\n",
    "data_rdd_with_tuples = data_rdd.map(lambda x: (x[0], tuple(x[1])))\n",
    "\n",
    "# Split the data into train and test sets in a stratified fashion\n",
    "train_rdd_with_tuples = data_rdd_with_tuples.sampleByKey(\n",
    "    withReplacement=False, \n",
    "    fractions={0.0: 0.8, 1.0: 0.8}, \n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Perform the subtract with hashable elements\n",
    "test_rdd_with_tuples = data_rdd_with_tuples.subtract(train_rdd_with_tuples)\n",
    "\n",
    "# Convert back to lists for further processing\n",
    "train_rdd = train_rdd_with_tuples.map(lambda x: (x[0], list(x[1])))\n",
    "test_rdd = test_rdd_with_tuples.map(lambda x: (x[0], list(x[1])))\n",
    "\n",
    "# Oversample the train RDD to deal with class imbalance\n",
    "# Calculate class counts in the training data\n",
    "count_dict = train_rdd.countByKey()\n",
    "major_count, minor_count = count_dict[0], count_dict[1]\n",
    "# Calculate the desired oversampling ratio\n",
    "ratio = float(major_count) / minor_count\n",
    "# Filter out and oversample the minor class\n",
    "oversampled_minor_rdd = train_rdd\\\n",
    "    .filter(lambda x : x[0] == 1)\\\n",
    "    .sample(withReplacement=True, fraction=ratio, seed=42)\n",
    "# Combine the oversampled minor with the train RDD\n",
    "train_rdd = train_rdd\\\n",
    "    .filter(lambda x : x[0] == 0)\\\n",
    "    .union(oversampled_minor_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement and train the model using low-level operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "class MyLogisticRegressionModel:\n",
    "    def __init__(self, learning_rate=5.0, num_iterations=50, convergence_tol=1e-4):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iterations = num_iterations\n",
    "        self.convergence_tol = convergence_tol\n",
    "        self.weights = None\n",
    "        self.converged_at = None\n",
    "        \n",
    "    def predictProb(self, features):\n",
    "        \"\"\"Computes the prediction for a single feature vector.\"\"\"\n",
    "        s = sum(w * f for w, f in zip(self.weights, features))\n",
    "        return 1 / (1 + exp(-s)) if s >= 0 else exp(s) / (1 + exp(s))\n",
    "    \n",
    "    def updateWeights(self, train_rdd, iteration):\n",
    "        \"\"\"Computes the gradient using mapPartitions for better performance.\"\"\"\n",
    "        # Use adaptive learning rate\n",
    "        current_lr = self.learning_rate / (1 + 0.01 * iteration)\n",
    "        \n",
    "        def process_partition(partition):\n",
    "            \"\"\"Process all records in a partition at once for better efficiency.\"\"\"\n",
    "            local_gradients = [0.0] * len(self.weights)\n",
    "            count = 0\n",
    "            \n",
    "            # Process all records in the partition\n",
    "            for label, features in partition:\n",
    "                # Calculate prediction\n",
    "                s = sum(w * f for w, f in zip(self.weights, features))\n",
    "                pred = 1 / (1 + exp(-s)) if s >= 0 else exp(s) / (1 + exp(s))\n",
    "                \n",
    "                # Update gradient for this record\n",
    "                error = pred - label\n",
    "                for i, feature in enumerate(features):\n",
    "                    local_gradients[i] += error * feature\n",
    "                count += 1\n",
    "            \n",
    "            # Return normalized gradients from this partition\n",
    "            if count > 0:\n",
    "                local_gradients = [g / count for g in local_gradients]\n",
    "            \n",
    "            yield local_gradients\n",
    "        \n",
    "        # Calculate gradients across all partitions\n",
    "        gradients = train_rdd\\\n",
    "            .mapPartitions(process_partition)\\\n",
    "            .reduce(lambda a, b: [x + y for x, y in zip(a, b)])\n",
    "        \n",
    "        # Update weights using current learning rate\n",
    "        updated_weights = [w - current_lr * g for w, g in zip(self.weights, gradients)]\n",
    "        return updated_weights\n",
    "    \n",
    "    def fit(self, train_rdd):\n",
    "        \"\"\"Fits the Logistic Regression model with optimizations.\"\"\"\n",
    "        # Add intercept and cache the data\n",
    "        train_rdd_with_intercept = train_rdd.map(lambda x: (x[0], [1.0] + x[1])).cache()\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.weights = [0.0] * len(train_rdd_with_intercept.first()[1])\n",
    "        \n",
    "        # Iterate until convergence or max iterations\n",
    "        for i in range(self.num_iterations):\n",
    "            # Update weights\n",
    "            new_weights = self.updateWeights(train_rdd_with_intercept, i)\n",
    "            \n",
    "            # Check for convergence\n",
    "            weight_diff = sum(abs(new - old) for new, old in zip(new_weights, self.weights))\n",
    "            self.weights = new_weights\n",
    "            \n",
    "            if weight_diff < self.convergence_tol * len(self.weights):\n",
    "                self.converged_at = i\n",
    "                break\n",
    "                \n",
    "        if not hasattr(self, 'converged_at') or self.converged_at is None:\n",
    "            self.converged_at = self.num_iterations\n",
    "            \n",
    "        # Clean up cached RDD\n",
    "        train_rdd_with_intercept.unpersist()\n",
    "        \n",
    "    def predict(self, features):\n",
    "        \"\"\"Predicts class label.\"\"\"\n",
    "        features_with_intercept = [1.0] + features\n",
    "        return 0.0 if self.predictProb(features_with_intercept) < 0.5 else 1.0\n",
    "        \n",
    "# Initialize and train with optimized model\n",
    "model = MyLogisticRegressionModel(learning_rate=5.0, num_iterations=50, convergence_tol=1e-4)\n",
    "model.fit(train_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [-3.4292099748116573, -9.802899192074307, 9.696726839609024, -27.818595614355036, 21.408690192345475, -19.53336699438556, -7.407869845508147, -39.40542508060768, 4.935976623038762, -24.352598367969605, -53.109896020676814, 35.28457868072276, -64.36455530481766, -0.46322655112228733, -66.45665150061443, -1.5828028978673325, -58.489120381694086, -104.56813100275232, -38.46713176496969, 14.084508086021499, 4.802675362058377, 9.36926524675539, 2.5781994898848266, -2.274474631475483, 0.9319434869757415, -0.5558624105814621, -3.6953273573100223, 10.241343912665242, 5.000428271361841, 0.039141697375986634]\n",
      "Intercept: -6.866025234934795\n",
      "Converged at: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/keineik/Projects/lab03-spark-ml/venv/lib64/python3.13/site-packages/pyspark/sql/context.py:158: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9710\n",
      "Precision by label: [0: 0.9998, 1: 0.0491]\n",
      "Recall by label: [0: 0.9711, 1: 0.9032]\n",
      "Area under ROC: 0.9785\n",
      "Area under PR: 0.4404\n"
     ]
    }
   ],
   "source": [
    "# Import BinaryClassificationMetrics for ROC calculation\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "print(\"Coefficients:\", model.weights[1:])\n",
    "print(\"Intercept:\", model.weights[0])\n",
    "print(\"Converged at:\", model.converged_at)\n",
    "\n",
    "# Get predictions and labels for evaluation\n",
    "predictionAndLabels = test_rdd.map(lambda p: (model.predict(p[1]), p[0]))\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = predictionAndLabels.filter(lambda x: x[0] == x[1]).count() / predictionAndLabels.count()\n",
    "\n",
    "# Calculate confusion matrix values\n",
    "true_positives = predictionAndLabels.filter(lambda x: x[0] == 1.0 and x[1] == 1.0).count()\n",
    "false_positives = predictionAndLabels.filter(lambda x: x[0] == 1.0 and x[1] == 0.0).count()\n",
    "true_negatives = predictionAndLabels.filter(lambda x: x[0] == 0.0 and x[1] == 0.0).count()\n",
    "false_negatives = predictionAndLabels.filter(lambda x: x[0] == 0.0 and x[1] == 1.0).count()\n",
    "\n",
    "# Calculate precision for each label\n",
    "precision_0 = true_negatives / (true_negatives + false_negatives) if (true_negatives + false_negatives) > 0 else 0\n",
    "precision_1 = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "\n",
    "# Calculate recall for each label\n",
    "recall_0 = true_negatives / (true_negatives + false_positives) if (true_negatives + false_positives) > 0 else 0\n",
    "recall_1 = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "\n",
    "# For ROC calculation, we need prediction scores (probabilities) instead of binary predictions\n",
    "# Create RDD with (score, label) format required by BinaryClassificationMetrics\n",
    "scoreAndLabels = test_rdd.map(lambda x: (model.predictProb([1.0] + x[1]), x[0]))\n",
    "\n",
    "# Create binary classification metrics object\n",
    "metrics = BinaryClassificationMetrics(scoreAndLabels)\n",
    "\n",
    "# Get area under ROC\n",
    "auc_roc = metrics.areaUnderROC\n",
    "auc_pr = metrics.areaUnderPR\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision by label: [0: {precision_0:.4f}, 1: {precision_1:.4f}]\")\n",
    "print(f\"Recall by label: [0: {recall_0:.4f}, 1: {recall_1:.4f}]\")\n",
    "print(f\"Area under ROC: {auc_roc:.4f}\")\n",
    "print(f\"Area under PR: {auc_pr:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
