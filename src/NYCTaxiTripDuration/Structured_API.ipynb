{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"NYCTaxiTripDurationRegression\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 94:>                                                       (0 + 12) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-------------------+-------------------+---------------+------------------+------------------+------------------+------------------+------------------+-------------+\n",
      "|       id|vendor_id|    pickup_datetime|   dropoff_datetime|passenger_count|  pickup_longitude|   pickup_latitude| dropoff_longitude|  dropoff_latitude|store_and_fwd_flag|trip_duration|\n",
      "+---------+---------+-------------------+-------------------+---------------+------------------+------------------+------------------+------------------+------------------+-------------+\n",
      "|id2875421|        2|2016-03-14 17:24:55|2016-03-14 17:32:30|              1| -73.9821548461914| 40.76793670654297|-73.96463012695312|40.765602111816406|                 N|          455|\n",
      "|id2377394|        1|2016-06-12 00:43:35|2016-06-12 00:54:38|              1|-73.98041534423828|40.738563537597656|-73.99948120117188| 40.73115158081055|                 N|          663|\n",
      "|id3858529|        2|2016-01-19 11:35:24|2016-01-19 12:10:48|              1| -73.9790267944336|40.763938903808594|-74.00533294677734|40.710086822509766|                 N|         2124|\n",
      "|id3504673|        2|2016-04-06 19:32:31|2016-04-06 19:39:40|              1|-74.01004028320312|   40.719970703125|-74.01226806640625| 40.70671844482422|                 N|          429|\n",
      "|id2181028|        2|2016-03-26 13:30:55|2016-03-26 13:38:10|              1|-73.97305297851562|40.793209075927734| -73.9729232788086| 40.78252029418945|                 N|          435|\n",
      "+---------+---------+-------------------+-------------------+---------------+------------------+------------------+------------------+------------------+------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data = spark.read.csv(\"train.csv\", header=True, inferSchema=True)\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import hour, minute, dayofweek, month, sqrt, pow\n",
    "\n",
    "data = data.withColumn(\"pickup_minutes\", hour(\"pickup_datetime\") * 60 + minute(\"pickup_datetime\")) \\\n",
    "            .withColumn(\"pickup_dayofweek\", dayofweek(\"pickup_datetime\")) \\\n",
    "            .withColumn(\"pickup_month\", month(\"pickup_datetime\")) \\\n",
    "            .withColumn(\"distance\", sqrt(\n",
    "                pow(data[\"pickup_longitude\"] - data[\"dropoff_longitude\"], 2) +\n",
    "                pow(data[\"pickup_latitude\"] - data[\"dropoff_latitude\"], 2)\n",
    "            ))\n",
    "\n",
    "data = data.filter(\"passenger_count > 0\") \\\n",
    "            .filter(\"trip_duration < 22 * 3600\") \\\n",
    "            .filter(\"distance > 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "feature_columns = [\n",
    "    \"passenger_count\",\n",
    "    \"pickup_longitude\",\n",
    "    \"pickup_latitude\",\n",
    "    \"distance\",\n",
    "    \"pickup_minutes\",\n",
    "    \"pickup_dayofweek\",\n",
    "    \"pickup_month\",\n",
    "]\n",
    "\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "assembled_data = assembler.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "\n",
    "train, test = assembled_data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "train = train.select(\"features\", \"trip_duration\")\n",
    "\n",
    "test = test.select(\"features\", \"trip_duration\")\n",
    "\n",
    "dt = DecisionTreeRegressor(\n",
    "    featuresCol=\"features\", \n",
    "    labelCol=\"trip_duration\", \n",
    "    maxDepth=10,\n",
    "    minInstancesPerNode=4, \n",
    "    seed=42)\n",
    "\n",
    "model = dt.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+------------------+\n",
      "|            features|trip_duration|        prediction|\n",
      "+--------------------+-------------+------------------+\n",
      "|[1.0,-73.97711944...|         1134|1050.1267576221635|\n",
      "|[1.0,-73.97360992...|          592| 535.0468597461763|\n",
      "|[1.0,-73.96556091...|         1677|1396.4377101143023|\n",
      "|[1.0,-73.95832824...|          303|253.82171809212593|\n",
      "|[1.0,-73.98651123...|          189| 351.0723589001447|\n",
      "+--------------------+-------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on test data = 641.355633375002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 152:====>                                                  (1 + 11) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² on test data = 0.4316665906009668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "predictions = model.transform(test)\n",
    "predictions.show(5)\n",
    "\n",
    "# Evaluate RMSE\n",
    "rmse_evaluator = RegressionEvaluator(labelCol=\"trip_duration\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = rmse_evaluator.evaluate(predictions)\n",
    "print(\"RMSE on test data =\", rmse)\n",
    "\n",
    "# Evaluate R²\n",
    "r2_evaluator = RegressionEvaluator(labelCol=\"trip_duration\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "r2 = r2_evaluator.evaluate(predictions)\n",
    "print(\"R² on test data =\", r2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
