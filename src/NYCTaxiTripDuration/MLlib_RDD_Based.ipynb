{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/04/09 12:14:05 WARN Utils: Your hostname, jztr resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/04/09 12:14:05 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/04/09 12:14:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from math import sqrt, pow\n",
    "\n",
    "from pyspark.mllib.linalg import DenseVector\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.tree import DecisionTree\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "                    .appName(\"NYCTaxiTripDurationRegression\")\\\n",
    "                    .master(\"local[*]\")\\\n",
    "                    .config(\"spark.log.level\", \"ERROR\")\\\n",
    "                    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data as an RDD\n",
    "data_rdd = sc.textFile(\"../../../data/train.csv\")\n",
    "\n",
    "# Skip the header\n",
    "header = data_rdd.first()\n",
    "data_rdd = data_rdd.filter(lambda line: line != header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Columns**:\n",
    "  - `id`: Unique identifier for each trip.\n",
    "  - `vendor_id`: ID of the taxi vendor.\n",
    "  - `pickup_datetime` and `dropoff_datetime`: Timestamps for the start and end of the trip.\n",
    "  - `passenger_count`: Number of passengers in the taxi.\n",
    "  - `pickup_longitude` and `pickup_latitude`: GPS coordinates of the pickup location.\n",
    "  - `dropoff_longitude` and `dropoff_latitude`: GPS coordinates of the dropoff location.\n",
    "  - `store_and_fwd_flag`: Whether the trip record was held in the vehicle's memory before sending to the server (`Y` or `N`).\n",
    "  - `trip_duration`: Duration of the trip in seconds.\n",
    "\n",
    "1. **Feature Extraction**:\n",
    "  - Extracted additional features such as:\n",
    "    - `pickup_minutes`: Total minutes from the start of the day.\n",
    "    - `pickup_dayofweek`: Day of the week.\n",
    "    - `pickup_month`: Month of the year.\n",
    "    - `distance`: Euclidean distance between pickup and dropoff locations.\n",
    "\n",
    "2. **Filtering Invalid Data**:\n",
    "  - Removed trips with:\n",
    "    - `passenger_count` less than or equal to 0.\n",
    "    - `trip_duration` greater than 22 hours (extreme outliers).\n",
    "    - `distance` less than or equal to 0.\n",
    "\n",
    "3. **Feature Assembly**:\n",
    "  - Combined relevant features into a single vector using `VectorAssembler`. The selected features include:\n",
    "    - `passenger_count`\n",
    "    - `pickup_longitude`\n",
    "    - `pickup_latitude`\n",
    "    - `distance`\n",
    "    - `pickup_minutes`\n",
    "    - `pickup_dayofweek`\n",
    "    - `pickup_month`\n",
    "\n",
    "4. **Data Transformation**:\n",
    "  - Transformed the data into a format suitable for machine learning by creating a `features` column and retaining the target variable `trip_duration`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1,\n",
       "  40.76793670654297,\n",
       "  -73.9821548461914,\n",
       "  0.01767953949959892,\n",
       "  1044,\n",
       "  1,\n",
       "  3,\n",
       "  455]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_trip_line(line):\n",
    "    fields = line.split(\",\")\n",
    "    pickup_datetime = datetime.strptime(fields[2], \"%Y-%m-%d %H:%M:%S\") # pickup_datetime\n",
    "    passenger_count = int(fields[4]) # passenger_count\n",
    "    pickup_longitude = float(fields[5]) # pickup_longitude\n",
    "    pickup_latitude = float(fields[6]) # pickup_latitude\n",
    "    dropoff_longitude = float(fields[7]) # dropoff_longitude\n",
    "    dropoff_latitude = float(fields[8]) # dropoff_latitude\n",
    "    trip_duration = int(fields[10]) # trip_duration\n",
    "\n",
    "    pickup_minutes = pickup_datetime.hour * 60 + pickup_datetime.minute\n",
    "    pickup_dayofweek = pickup_datetime.weekday() + 1\n",
    "    pickup_month = pickup_datetime.month\n",
    "    distance = sqrt(pow((pickup_longitude - dropoff_longitude), 2) + pow((pickup_latitude - dropoff_latitude), 2))\n",
    "    return [\n",
    "        passenger_count,\n",
    "        pickup_latitude,\n",
    "        pickup_longitude,\n",
    "        distance,\n",
    "        pickup_minutes,\n",
    "        pickup_dayofweek,\n",
    "        pickup_month,\n",
    "        trip_duration\n",
    "    ]\n",
    "\n",
    "data_rdd = data_rdd.map(process_trip_line)\n",
    "data_rdd = data_rdd.filter(lambda x: x[0] > 0) \\\n",
    "                    .filter(lambda x: x[-1] < 22 * 3600) \\\n",
    "                    .filter(lambda x: x[3] > 0)\n",
    "                    \n",
    "data_rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to LabeledPoint RDD\n",
    "labeled_rdd = data_rdd.map(lambda x: LabeledPoint(x[-1], x[:-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training:\n",
    "  - Split the data into training and testing sets.\n",
    "  - Train a Decision Tree Regressor using the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# Split into training and test data\n",
    "train_rdd, test_rdd = labeled_rdd.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Train the DecisionTree model\n",
    "model = DecisionTree.trainRegressor(\n",
    "    train_rdd,\n",
    "    categoricalFeaturesInfo={},\n",
    "    maxDepth=10,\n",
    "    minInstancesPerNode=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation:\n",
    "  - Evaluate the model's performance on the test data using metrics such as RMSE and MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thanhthuy/spark/spark-3.3.2-bin-hadoop3/python/pyspark/sql/context.py:157: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n",
      "[Stage 34:>                                                         (0 + 6) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE): 644.460627552359\n",
      "R-squared (R²): 0.424092303496584\n",
      "Mean Absolute Error (MAE): 233.8591077432194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test set\n",
    "predictions = model.predict(test_rdd.map(lambda x: x.features))\n",
    "predictions_and_labels = predictions.zip(test_rdd.map(lambda lp: lp.label))\n",
    "\n",
    "# Evaluate the model\n",
    "metrics = RegressionMetrics(predictions_and_labels)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Root Mean Squared Error (RMSE):\", metrics.rootMeanSquaredError)\n",
    "print(\"R-squared (R²):\", metrics.r2)\n",
    "print(\"Mean Absolute Error (MAE):\", metrics.meanAbsoluteError)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
